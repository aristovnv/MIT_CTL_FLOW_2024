{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f6a75c7-7455-44ba-b3f9-711dc0549854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "userName = 'naristov'\n",
    "path = f\"/home/gridsan/{userName}/Modelling/Data\"\n",
    "\n",
    "shipsData = pd.read_csv(f\"{path}/shipsData.csv\")\n",
    "anchorStatsBoston = pd.read_csv(f\"{path}/ResultedAnchorStatsBoston.csv\")\n",
    "anchorStatsNY = pd.read_csv(f\"{path}/ResultedAnchorStatsNY.csv\")\n",
    "berthStatsBoston = pd.read_csv(f\"{path}/ResultedBerthStatsBoston.csv\")\n",
    "berthStatsNY = pd.read_csv(f\"{path}/ResultedBerthStatsNY.csv\")\n",
    "harborStatsBoston = pd.read_csv(f\"{path}/ResultedHarborStatsBoston.csv\")\n",
    "harborStatsNY = pd.read_csv(f\"{path}/ResultedHarborStatsNY.csv\")\n",
    "\n",
    "BerthMergeAnchor = pd.read_csv(f\"{path}/BerthMergeAnchor.csv\")\n",
    "BerthMergeAnchor = pd.merge(BerthMergeAnchor, shipsData,\n",
    "              how='left', left_on='MMSI', right_on='MMSI')\n",
    "\n",
    "# Create columns dynamically based on operators\n",
    "operator_columns = pd.get_dummies(BerthMergeAnchor['Operator'], prefix='operator', dummy_na=False)\n",
    "BerthMergeAnchor = pd.concat([BerthMergeAnchor, operator_columns], axis=1)\n",
    "BerthMergeAnchorBoston = BerthMergeAnchor[BerthMergeAnchor['groupedPort'] == 'Boston']\n",
    "BerthMergeAnchorNY = BerthMergeAnchor[BerthMergeAnchor['groupedPort'] == 'NY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6bab7f-3c0d-4538-b40d-d02eedf2848a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458aaeab-75b5-4640-945a-846d8ceee20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terminal APM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/naristov/.local/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "/home/gridsan/naristov/.local/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =            5     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  7.44982D+00    |proj g|=  7.69069D-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate    5    f=  7.37979D+00    |proj g|=  9.58174D-02\n",
      "\n",
      "At iterate   10    f=  7.35456D+00    |proj g|=  1.89496D-03\n",
      "\n",
      "At iterate   15    f=  7.34214D+00    |proj g|=  3.66569D-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " Bad direction in the line search;\n",
      "   refresh the lbfgs memory and restart the iteration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   20    f=  7.33282D+00    |proj g|=  3.70583D-03\n",
      "\n",
      "At iterate   25    f=  7.33281D+00    |proj g|=  9.14287D-04\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "    5     25     58      2     0     0   9.143D-04   7.333D+00\n",
      "  F =   7.3328103938504130     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/naristov/.local/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:836: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
      "  return get_prediction_index(\n",
      "/home/gridsan/naristov/.local/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:836: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n",
      "  return get_prediction_index(\n",
      "/home/gridsan/naristov/.local/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "/home/gridsan/naristov/.local/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =            5     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.74185D+00    |proj g|=  2.38434D-01\n",
      "\n",
      "At iterate    5    f=  1.53743D+00    |proj g|=  9.90419D-02\n",
      "\n",
      "At iterate   10    f=  1.49549D+00    |proj g|=  1.10520D-02\n",
      "\n",
      "At iterate   15    f=  1.48879D+00    |proj g|=  4.59983D-03\n",
      "\n",
      "At iterate   20    f=  1.48744D+00    |proj g|=  7.48215D-03\n",
      "\n",
      "At iterate   25    f=  1.48738D+00    |proj g|=  1.20473D-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " Bad direction in the line search;\n",
      "   refresh the lbfgs memory and restart the iteration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "    5     27     88      2     0     0   1.620D-02   1.487D+00\n",
      "  F =   1.4873827681855496     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " Warning:  more than 10 function and gradient\n",
      "   evaluations in the last line search.  Termination\n",
      "   may possibly be caused by a bad search direction.\n",
      "/home/gridsan/naristov/.local/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:836: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
      "  return get_prediction_index(\n",
      "/home/gridsan/naristov/.local/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:836: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n",
      "  return get_prediction_index(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terminal LibertyB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/naristov/.local/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "/home/gridsan/naristov/.local/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =            5     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  7.09540D+00    |proj g|=  7.62549D-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate    5    f=  7.02390D+00    |proj g|=  3.71978D-02\n",
      "\n",
      "At iterate   10    f=  6.99844D+00    |proj g|=  1.65746D-03\n",
      "\n",
      "At iterate   15    f=  6.96286D+00    |proj g|=  8.38186D-02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "    5     16     42      1     0     0   8.373D-02   6.963D+00\n",
      "  F =   6.9628642670947496     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " Warning:  more than 10 function and gradient\n",
      "   evaluations in the last line search.  Termination\n",
      "   may possibly be caused by a bad search direction.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Assuming you have three dataframes df_terminal, df_harbor, and df_traffic_rate\n",
    "\n",
    "df_pivoted = berthStatsNY.pivot_table(index='Hour', columns='nearestPort', values=['count', 'Length'], aggfunc='sum', fill_value=0)\n",
    "\n",
    "# Flatten the multi-level columns\n",
    "df_pivoted.columns = [f'{agg}_{terminal}' for agg, terminal in df_pivoted.columns]\n",
    "\n",
    "# Reset the index to make 'hour' a regular column\n",
    "df_pivoted.reset_index(inplace=True)\n",
    "# Merge the dataframes on the common time-related column (hour)\n",
    "df_merged = pd.merge(df_pivoted, anchorStatsNY, on='Hour', how='left', suffixes=('', '_anchor'))\n",
    "df_merged = pd.merge(df_merged, harborStatsNY, on='Hour', how='left', suffixes=('', '_harbor'))\n",
    "\n",
    "# Handle missing values if any\n",
    "df_merged.fillna(0, inplace=True)  # You may need a different strategy based on your data\n",
    "\n",
    "# Feature Engineering\n",
    "df_merged['Hour']= pd.to_datetime(df_merged['Hour'])\n",
    "df_merged['day_of_week'] = df_merged['Hour'].dt.day_of_week\n",
    "df_merged['month'] = df_merged['Hour'].dt.month\n",
    "\n",
    "df_merged['Hour']= df_merged['Hour'].astype(int)\n",
    "df_merged.drop(['Unnamed: 0.1', 'index', 'Unnamed: 0','Unnamed: 0.1_harbor','index_harbor','Unnamed: 0_anchor','Unnamed: 0_harbor' ], axis = 1, inplace=True)\n",
    "# Optionally, add information about vessels' operators if available\n",
    "# df_merged = pd.merge(df_merged, df_operators, on='hour', how='left')\n",
    "#print(df_merged.info())\n",
    "# Modeling\n",
    "\n",
    "# Assuming df is your DataFrame with the provided columns\n",
    "\n",
    "# Select relevant columns for features and targets\n",
    "features_columns = ['Hour', 'Length_NY_APM', 'Length_NY_LibertyB', 'Length_NY_LibertyNY', 'Length_NY_Maher',\n",
    "                    'Length_NY_Newark', 'Length_NY_RedHook', 'count_NY_APM', 'count_NY_LibertyB', \n",
    "                    'count_NY_LibertyNY', 'count_NY_Maher', 'count_NY_Newark', 'count_NY_RedHook',\n",
    "                    'count', 'count_harbor', 'Length', 'Width', 'day_of_week', 'month']\n",
    "\n",
    "#target_columns = ['Length_NY_APM', 'Length_NY_LibertyB', 'Length_NY_LibertyNY', 'Length_NY_Maher',\n",
    "#                  'Length_NY_Newark', 'Length_NY_RedHook', 'count_NY_APM', 'count_NY_LibertyB',\n",
    "#                  'count_NY_LibertyNY', 'count_NY_Maher', 'count_NY_Newark', 'count_NY_RedHook']\n",
    "\n",
    "terminals = ['APM', 'LibertyB', 'LibertyNY', 'Maher', 'Newark', 'RedHook']\n",
    "\n",
    "# Initialize empty DataFrames to store forecasts\n",
    "length_forecasts = pd.DataFrame(index=df_merged['Hour'])\n",
    "count_forecasts = pd.DataFrame(index=df_merged['Hour'])                  \n",
    "\n",
    "for terminal in terminals:\n",
    "    print(f\"terminal {terminal}\")\n",
    "    # Select relevant columns for features and targets\n",
    "    target_columns = [f'Length_NY_{terminal}', f'count_NY_{terminal}']\n",
    "\n",
    "    # Extract features and targets\n",
    "    X = df_merged[features_columns]\n",
    "    y = df_merged[target_columns]\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Fit univariate SARIMAX model for length\n",
    "    model_length = SARIMAX(y_train[f'Length_NY_{terminal}'], order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\n",
    "    result_length = model_length.fit()\n",
    "\n",
    "    # Forecast length for the current terminal\n",
    "    length_forecasts[f'Length_NY_{terminal}'] = result_length.get_forecast(steps=len(X_test)).predicted_mean\n",
    "\n",
    "    # Fit univariate SARIMAX model for count\n",
    "    model_count = SARIMAX(y_train[f'count_NY_{terminal}'], order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\n",
    "    result_count = model_count.fit()\n",
    "\n",
    "    # Forecast count for the current terminal\n",
    "    count_forecasts[f'count_NY_{terminal}'] = result_count.get_forecast(steps=len(X_test)).predicted_mean\n",
    "\n",
    "# Display or further analyze the forecasts\n",
    "print(length_forecasts)\n",
    "print(count_forecasts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a409eb05-d660-4f0c-aa19-8bff2fd73430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf952d2-0c3f-45b1-83ec-ed8fdf0a06bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4307a2-47b3-4c4b-96c4-c4053e5192ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pmdarima import auto_arima\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df is your DataFrame with the provided columns\n",
    "\n",
    "# List of terminal prefixes\n",
    "terminals = ['APM', 'LibertyB', 'LibertyNY', 'Maher', 'Newark', 'RedHook']\n",
    "\n",
    "# Extend the time index for future forecasting\n",
    "future_index = pd.date_range(start=df['Hour'].max() + pd.Timedelta(hours=1), periods=24, freq='H')\n",
    "\n",
    "# Initialize empty DataFrames to store forecasts\n",
    "length_forecasts = pd.DataFrame(index=future_index)\n",
    "count_forecasts = pd.DataFrame(index=future_index)\n",
    "\n",
    "# Initialize dictionaries to store results for comparison\n",
    "mse_results = {'AutoARIMA': {}, 'ExponentialSmoothing': {}, 'LSTM': {}}\n",
    "\n",
    "# Loop through each terminal\n",
    "for terminal in terminals:\n",
    "    # Select relevant columns for features and targets\n",
    "    features_columns = ['Hour']  # Add other relevant features here\n",
    "    target_columns = [f'Length_NY_{terminal}', f'count_NY_{terminal}']\n",
    "\n",
    "    # Extract features and targets\n",
    "    X = df[features_columns]\n",
    "    y = df[target_columns]\n",
    "\n",
    "    # AutoARIMA\n",
    "    model_autoarima = auto_arima(y[f'Length_NY_{terminal}'], exogenous=X, suppress_warnings=True)\n",
    "    length_forecast_autoarima = model_autoarima.predict(n_periods=24, exogenous=X.tail(24))\n",
    "\n",
    "    model_autoarima_count = auto_arima(y[f'count_NY_{terminal}'], exogenous=X, suppress_warnings=True)\n",
    "    count_forecast_autoarima = model_autoarima_count.predict(n_periods=24, exogenous=X.tail(24))\n",
    "\n",
    "    length_forecasts[f'Length_NY_{terminal}_AutoARIMA'] = length_forecast_autoarima\n",
    "    count_forecasts[f'count_NY_{terminal}_AutoARIMA'] = count_forecast_autoarima\n",
    "\n",
    "    mse_results['AutoARIMA'][f'Length_NY_{terminal}'] = mean_squared_error(y[f'Length_NY_{terminal}'], length_forecast_autoarima)\n",
    "    mse_results['AutoARIMA'][f'count_NY_{terminal}'] = mean_squared_error(y[f'count_NY_{terminal}'], count_forecast_autoarima)\n",
    "\n",
    "    # Exponential Smoothing\n",
    "    model_es_length = ExponentialSmoothing(y[f'Length_NY_{terminal}'], trend='add', seasonal='add', seasonal_periods=24)\n",
    "    result_es_length = model_es_length.fit()\n",
    "    length_forecast_es = result_es_length.predict(start=y.index[-1] + pd.Timedelta(hours=1), end=y.index[-1] + pd.Timedelta(hours=24))\n",
    "\n",
    "    model_es_count = ExponentialSmoothing(y[f'count_NY_{terminal}'], trend='add', seasonal='add', seasonal_periods=24)\n",
    "    result_es_count = model_es_count.fit()\n",
    "    count_forecast_es = result_es_count.predict(start=y.index[-1] + pd.Timedelta(hours=1), end=y.index[-1] + pd.Timedelta(hours=24))\n",
    "\n",
    "    length_forecasts[f'Length_NY_{terminal}_ExponentialSmoothing'] = length_forecast_es\n",
    "    count_forecasts[f'count_NY_{terminal}_ExponentialSmoothing'] = count_forecast_es\n",
    "\n",
    "    mse_results['ExponentialSmoothing'][f'Length_NY_{terminal}'] = mean_squared_error(y[f'Length_NY_{terminal}'], length_forecast_es)\n",
    "    mse_results['ExponentialSmoothing'][f'count_NY_{terminal}'] = mean_squared_error(y[f'count_NY_{terminal}'], count_forecast_es)\n",
    "\n",
    "    # LSTM (Long Short-Term Memory)\n",
    "    # Assuming you have standardized features and targets\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    y_scaled = scaler.fit_transform(y)\n",
    "\n",
    "    # LSTM model for length\n",
    "    model_lstm_length = Sequential()\n",
    "    model_lstm_length.add(LSTM(50, activation='relu', input_shape=(X_scaled.shape[1], 1)))\n",
    "    model_lstm_length.add(Dense(1))\n",
    "    model_lstm_length.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    model_lstm_length.fit(X_scaled, y_scaled[f'Length_NY_{terminal}'], epochs=50, verbose=0)\n",
    "\n",
    "    length_forecast_lstm = model_lstm_length.predict(X_scaled[-24:].reshape(1, 24, 1))\n",
    "    length_forecast_lstm = scaler.inverse_transform(length_forecast_lstm.reshape(-1, 1))\n",
    "\n",
    "    # LSTM model for count\n",
    "    model_lstm_count = Sequential()\n",
    "    model_lstm_count.add(LSTM(50, activation='relu', input_shape=(X_scaled.shape[1], 1)))\n",
    "    model_lstm_count.add(Dense(1))\n",
    "    model_lstm_count.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    model_lstm_count.fit(X_scaled, y_scaled[f'count_NY_{terminal}'], epochs=50, verbose=0)\n",
    "\n",
    "    count_forecast_lstm = model_lstm_count.predict(X_scaled[-24:].reshape(1, 24, 1))\n",
    "    count_forecast_lstm = scaler.inverse_transform(count_forecast_lstm.reshape(-1, 1))\n",
    "\n",
    "    length_forecasts[f'Length_NY_{terminal}_LSTM'] = length_forecast_lstm\n",
    "    count_forecasts[f'count_NY_{terminal}_LSTM'] = count_forecast_lstm\n",
    "\n",
    "    mse_results['LSTM'][f'Length_NY_{terminal}'] = mean_squared_error(y[f'Length_NY_{terminal}'], length_forecast_lstm)\n",
    "    mse_results['LSTM'][f'count_NY_{terminal}'] = mean_squared_error(y[f'count_NY_{terminal}'], count_forecast_lstm)\n",
    "\n",
    "# Print MSE results\n",
    "for method, errors in mse_results.items():\n",
    "    print(f\"\\nMethod: {method}\")\n",
    "    for variable, mse in errors.items():\n",
    "        print(f\"{variable}: {mse}\")\n",
    "\n",
    "# Plot forecasts for a specific terminal (change 'APM' to your desired terminal)\n",
    "terminal_to_plot = 'APM'\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Plot Length forecasts\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(df['Hour'], y[f'Length_NY_{terminal_to_plot}'], label=f'Actual Length {terminal_to_plot}', linestyle='--', marker='o')\n",
    "for method in mse_results.keys():\n",
    "    plt.plot(future_index, length_forecasts[f'Length_NY_{terminal_to_plot}_{method}'], label=f'{method} Forecast')\n",
    "plt.title(f'Length Forecast Comparison for Terminal {terminal_to_plot}')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Count forecasts\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(df['Hour'], y[f'count_NY_{terminal_to_plot}'], label=f'Actual Count {terminal_to_plot}', linestyle='--', marker='o')\n",
    "for method in mse_results.keys():\n",
    "    plt.plot(future_index, count_forecasts[f'count_NY_{terminal_to_plot}_{method}'], label=f'{method} Forecast')\n",
    "plt.title(f'Count Forecast Comparison for Terminal {terminal_to_plot}')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f8bde0-b049-4dfc-9906-9f72f6d578af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def fit_autoarima(target_series, exog_data, trace=False):\n",
    "    return auto_arima(target_series, exogenous=exog_data, suppress_warnings=True, trace=trace)\n",
    "\n",
    "# Assuming df is your DataFrame with the provided columns\n",
    "\n",
    "# List of terminal prefixes\n",
    "terminals = ['APM', 'LibertyB', 'LibertyNY', 'Maher', 'Newark', 'RedHook']\n",
    "\n",
    "# Initialize empty DataFrames to store forecasts\n",
    "length_forecasts = pd.DataFrame(index=future_index)\n",
    "count_forecasts = pd.DataFrame(index=future_index)\n",
    "\n",
    "# Initialize dictionaries to store results for comparison\n",
    "mse_results = {'AutoARIMA': {}}\n",
    "\n",
    "# Loop through each terminal\n",
    "for terminal in terminals:\n",
    "    # Select relevant columns for features and targets\n",
    "    features_columns = ['Hour']  # Add other relevant features here\n",
    "    target_columns = [f'Length_NY_{terminal}', f'count_NY_{terminal}']\n",
    "\n",
    "    # Extract features and targets\n",
    "    X = df[features_columns]\n",
    "    y = df[target_columns]\n",
    "\n",
    "    # Parallel processing for AutoARIMA\n",
    "    results = Parallel(n_jobs=-1)(delayed(fit_autoarima)(y[col], X, trace=False) for col in target_columns)\n",
    "\n",
    "    # Unpack results\n",
    "    model_autoarima_length, model_autoarima_count = results\n",
    "\n",
    "    length_forecast_autoarima = model_autoarima_length.predict(n_periods=24, exogenous=X.tail(24))\n",
    "    count_forecast_autoarima = model_autoarima_count.predict(n_periods=24, exogenous=X.tail(24))\n",
    "\n",
    "    length_forecasts[f'Length_NY_{terminal}_AutoARIMA'] = length_forecast_autoarima\n",
    "    count_forecasts[f'count_NY_{terminal}_AutoARIMA'] = count_forecast_autoarima\n",
    "\n",
    "    mse_results['AutoARIMA'][f'Length_NY_{terminal}'] = mean_squared_error(y[f'Length_NY_{terminal}'], length_forecast_autoarima)\n",
    "    mse_results['AutoARIMA'][f'count_NY_{terminal}'] = mean_squared_error(y[f'count_NY_{terminal}'], count_forecast_autoarima)\n",
    "\n",
    "# Print MSE results\n",
    "for method, errors in mse_results.items():\n",
    "    print(f\"\\nMethod: {method}\")\n",
    "    for variable, mse in errors.items():\n",
    "        print(f\"{variable}: {mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a009267a-f384-4911-b961-70ab281bd792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pmdarima import auto_arima\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df is your DataFrame with the provided columns\n",
    "\n",
    "# Split data into train and test\n",
    "train_size = int(0.8 * len(df))\n",
    "train, test = df.iloc[:train_size], df.iloc[train_size:]\n",
    "\n",
    "length_forecasts = pd.DataFrame(index=test.index)\n",
    "count_forecasts = pd.DataFrame(index=test.index)\n",
    "# List of terminal prefixes\n",
    "terminals = ['APM', 'LibertyB', 'LibertyNY', 'Maher', 'Newark', 'RedHook']\n",
    "\n",
    "# Initialize empty DataFrames to store forecasts\n",
    "#length_forecasts = pd.DataFrame(index=test.index)\n",
    "#count_forecasts = pd.DataFrame(index=test.index)\n",
    "\n",
    "# Initialize dictionaries to store results for comparison\n",
    "mse_results = {'AutoARIMA': {}, 'ExponentialSmoothing': {}, 'LSTM': {}}\n",
    "\n",
    "# Loop through each terminal\n",
    "for terminal in terminals:\n",
    "    # Select relevant columns for features and targets\n",
    "    features_columns = ['Hour']  # Add other relevant features here\n",
    "    target_columns = [f'Length_NY_{terminal}', f'count_NY_{terminal}']\n",
    "\n",
    "    # Extract features and targets for train and test sets\n",
    "    X_train, y_train = train[features_columns], train[target_columns]\n",
    "    X_test, y_test = test[features_columns], test[target_columns]\n",
    "\n",
    "    # AutoARIMA\n",
    "    print(f\"AutoARIMA - Forecasting for Length_NY_{terminal}\")\n",
    "    model_autoarima_length = auto_arima(y_train[f'Length_NY_{terminal}'], exogenous=X_train, suppress_warnings=True, trace=True)\n",
    "    length_forecast_autoarima = model_autoarima_length.predict(n_periods=len(X_test), exogenous=X_test)\n",
    "\n",
    "    print(f\"AutoARIMA - Forecasting for count_NY_{terminal}\")\n",
    "    model_autoarima_count = auto_arima(y_train[f'count_NY_{terminal}'], exogenous=X_train, suppress_warnings=True, trace=True)\n",
    "    count_forecast_autoarima = model_autoarima_count.predict(n_periods=len(X_test), exogenous=X_test)\n",
    "\n",
    "    length_forecasts[f'Length_NY_{terminal}_AutoARIMA'] = length_forecast_autoarima\n",
    "    count_forecasts[f'count_NY_{terminal}_AutoARIMA'] = count_forecast_autoarima\n",
    "\n",
    "    mse_results['AutoARIMA'][f'Length_NY_{terminal}'] = mean_squared_error(y_test[f'Length_NY_{terminal}'], length_forecast_autoarima)\n",
    "    mse_results['AutoARIMA'][f'count_NY_{terminal}'] = mean_squared_error(y_test[f'count_NY_{terminal}'], count_forecast_autoarima)\n",
    "\n",
    "    # Exponential Smoothing\n",
    "    print(f\"Exponential Smoothing - Forecasting for Length_NY_{terminal}\")\n",
    "    model_es_length = ExponentialSmoothing(y_train[f'Length_NY_{terminal}'], trend='add', seasonal='add', seasonal_periods=24)\n",
    "    result_es_length = model_es_length.fit()\n",
    "    length_forecast_es = result_es_length.predict(start=len(y_train), end=len(y_train) + len(y_test) - 1)\n",
    "\n",
    "    print(f\"Exponential Smoothing - Forecasting for count_NY_{terminal}\")\n",
    "    model_es_count = ExponentialSmoothing(y_train[f'count_NY_{terminal}'], trend='add', seasonal='add', seasonal_periods=24)\n",
    "    result_es_count = model_es_count.fit()\n",
    "    count_forecast_es = result_es_count.predict(start=len(y_train), end=len(y_train) + len(y_test) - 1)\n",
    "\n",
    "    length_forecasts[f'Length_NY_{terminal}_ExponentialSmoothing'] = length_forecast_es\n",
    "    count_forecasts[f'count_NY_{terminal}_ExponentialSmoothing'] = count_forecast_es\n",
    "\n",
    "    mse_results['ExponentialSmoothing'][f'Length_NY_{terminal}'] = mean_squared_error(y_test[f'Length_NY_{terminal}'], length_forecast_es)\n",
    "    mse_results['ExponentialSmoothing'][f'count_NY_{terminal}'] = mean_squared_error(y_test[f'count_NY_{terminal}'], count_forecast_es)\n",
    "\n",
    "    # LSTM (Long Short-Term Memory)\n",
    "    print(f\"LSTM - Forecasting for Length_NY_{terminal}\")\n",
    "    # Assuming you have standardized features and targets\n",
    "    scaler_length = StandardScaler()\n",
    "    X_train_scaled_length = scaler_length.fit_transform(X_train)\n",
    "    X_test_scaled_length = scaler_length.transform(X_test)\n",
    "\n",
    "    model_lstm_length = Sequential()\n",
    "    model_lstm_length.add(LSTM(50, activation='relu', input_shape=(X_train_scaled_length.shape[1], 1)))\n",
    "    model_lstm_length.add(Dense(1))\n",
    "    model_lstm_length.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    model_lstm_length.fit(X_train_scaled_length, y_train[f'Length_NY_{terminal}'], epochs=50, verbose=0)\n",
    "\n",
    "    length_forecast_lstm = model_lstm_length.predict(X_test_scaled_length.reshape(1, len(X_test_scaled_length), 1))\n",
    "    length_forecast_lstm = scaler_length.inverse_transform(length_forecast_lstm.reshape(-1, 1))\n",
    "\n",
    "    print(f\"LSTM - Forecasting for count_NY_{terminal}\")\n",
    "    # Assuming you have standardized features and targets\n",
    "    scaler_count = StandardScaler()\n",
    "    X_train_scaled_count = scaler_count.fit_transform(X_train)\n",
    "    X_test_scaled_count = scaler_count.transform(X_test)\n",
    "\n",
    "    model_lstm_count = Sequential()\n",
    "    model_lstm_count.add(LSTM(50, activation='relu', input_shape=(X_train_scaled_count.shape[1], 1)))\n",
    "    model_lstm_count.add(Dense(1))\n",
    "    model_lstm_count.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    model_lstm_count.fit(X_train_scaled_count, y_train[f'count_NY_{terminal}'], epochs=50, verbose=0)\n",
    "\n",
    "    count_forecast_lstm = model_lstm_count.predict(X_test_scaled_count.reshape(1, len(X_test_scaled_count), 1))\n",
    "    count_forecast_lstm = scaler_count.inverse_transform(count_forecast_lstm.reshape(-1, 1))\n",
    "\n",
    "    length_forecasts[f'Length_NY_{terminal}_LSTM'] = length_forecast_lstm\n",
    "    count_forecasts[f'count_NY_{terminal}_LSTM'] = count_forecast_lstm\n",
    "\n",
    "    mse_results['LSTM'][f'Length_NY_{terminal}'] = mean_squared_error(y_test[f'Length_NY_{terminal}'], length_forecast_lstm)\n",
    "    mse_results['LSTM'][f'count_NY_{terminal}'] = mean_squared_error(y_test[f'count_NY_{terminal}'], count_forecast_lstm)\n",
    "\n",
    "# Print MSE results\n",
    "for method, errors in mse_results.items():\n",
    "    print(f\"\\nMethod: {method}\")\n",
    "    for variable, mse in errors.items():\n",
    "        print(f\"{variable}: {mse}\")\n",
    "\n",
    "# Save forecasts to CSV\n",
    "length_forecasts.to_csv('length_forecasts.csv', index=True)\n",
    "count_forecasts.to_csv('count_forecasts.csv', index=True)\n",
    "\n",
    "# Save MSE results to CSV\n",
    "mse_results_df = pd.DataFrame(mse_results)\n",
    "mse_results_df.to_csv('mse_results.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f68c75-2023-4c13-ac06-6ebb62d4410c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from pmdarima import auto_arima\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Assuming you have already trained models for each terminal\n",
    "\n",
    "# Save AutoARIMA models\n",
    "for terminal in terminals:\n",
    "    model_autoarima_length.save(f'model_autoarima_length_{terminal}.joblib')\n",
    "    model_autoarima_count.save(f'model_autoarima_count_{terminal}.joblib')\n",
    "\n",
    "# Save Exponential Smoothing models\n",
    "for terminal in terminals:\n",
    "    dump(model_es_length, f'model_es_length_{terminal}.joblib')\n",
    "    dump(model_es_count, f'model_es_count_{terminal}.joblib')\n",
    "\n",
    "# Save LSTM models\n",
    "for terminal in terminals:\n",
    "    model_lstm_length.save(f'model_lstm_length_{terminal}.h5')\n",
    "    model_lstm_count.save(f'model_lstm_count_{terminal}.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8064d77d-3ab4-474f-b6f6-82a3db7057da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from pmdarima import auto_arima\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load AutoARIMA models\n",
    "# Load the saved model from file\n",
    "with open('auto_arima_model.pkl', 'rb') as model_file:\n",
    "    loaded_model_auto_arima = pickle.load(model_file)\n",
    "\n",
    "# Now, you can use loaded_model_auto_arima for predictions\n",
    "\n",
    "\n",
    "# Load Exponential Smoothing models\n",
    "es_models_length = [load(f'model_es_length_{terminal}.joblib') for terminal in terminals]\n",
    "es_models_count = [load(f'model_es_count_{terminal}.joblib') for terminal in terminals]\n",
    "\n",
    "# Load LSTM models\n",
    "lstm_models_length = [load_model(f'model_lstm_length_{terminal}.h5') for terminal in terminals]\n",
    "lstm_models_count = [load_model(f'model_lstm_count_{terminal}.h5') for terminal in terminals]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1688605-ebd9-4237-98fd-7a053746bc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.40566943470639794\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pmdarima import auto_arima\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from joblib import dump\n",
    "from tensorflow.keras.models import load_model\n",
    "from joblib import load\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "import NYStatModel as ny\n",
    "\n",
    "\n",
    "def get_initial_data():\n",
    "    df_merged, features_columns = ny.dataPreparation()\n",
    "    train_size = int(0.8 * len(df_merged))\n",
    "    train, test = df_merged.iloc[:train_size], df_merged.iloc[train_size:]\n",
    "    return df_merged\n",
    "#for now assume we have this as forecast \n",
    "\n",
    "df_stats = get_initial_data()\n",
    "df = ny.BerthMergeAnchorNY.copy()\n",
    "df['start_time'] = pd.to_datetime(df['start_time'])\n",
    "df['end_time'] = pd.to_datetime(df['end_time'])\n",
    "df['start_time_anchor'] = pd.to_datetime(df['start_time_anchor'])\n",
    "df['end_time_anchor'] = pd.to_datetime(df['end_time_anchor'])\n",
    "\n",
    "df['isAnchor'] = np.where(df['start_time_anchor'].isnull(), 0 , 1)\n",
    "\n",
    "# Extract hour from 'start_time' if 'start_time_anchor' is null, else use 'start_time_anchor'\n",
    "df['Hour'] = np.where(df['start_time_anchor'].isnull(), df['start_time'], df['start_time_anchor'])\n",
    "df['Hour'] = pd.to_datetime(df['Hour'])\n",
    "df['Hour'] = pd.to_datetime(df['Hour'].dt.strftime('%Y-%m-%d %H:00:00'))\n",
    "# Subtract 2 hours from 'Hour'\n",
    "df['Hour'] = df['Hour'] - pd.to_timedelta(2, unit='H') \n",
    "# Convert 'Hour' to datetime format\n",
    "df['Hour'] = df['Hour'].astype(int)\n",
    "\n",
    "merged_df = pd.merge(df, df_stats, on='Hour', how='left')\n",
    "\n",
    "# Create a label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the terminal variable\n",
    "merged_df['terminal_encoded'] = label_encoder.fit_transform(merged_df['nearestPort'])\n",
    "\n",
    "merged_df.drop(['Unnamed: 0.2_x','index','Unnamed: 0.1_x','Unnamed: 0_x','groupedPort', 'Unnamed: 0.1_anchor', 'Unnamed: 0_anchor','VesselName_anchor','nearestPort_anchor', 'groupedPort_anchor','Unnamed: 0_anchor.1', 'index_anchor','nearestPort_anchor.1','group','start_time_anchor.1','end_time_anchor.1','TimeSpent_anchor.1','Unnamed: 0.8','Unnamed: 0.7','Unnamed: 0.6','Unnamed: 0.5','Unnamed: 0.4','Unnamed: 0.3','Unnamed: 0.2_y','Unnamed: 0.1_y','Unnamed: 0_y','Call Sign','Cargo Type','IMO Number','Name','Operator','Registered Owner','Ship Type','Vessel Type'], axis = 1, inplace = True) \n",
    "\n",
    "merged_df['start_time_anchor_int'] = merged_df['start_time_anchor'].astype(int) \n",
    "merged_df['end_time_anchor_int'] = merged_df['end_time_anchor'].astype(int)\n",
    "\n",
    "# Assuming you have a target variable 'terminal'\n",
    "X = merged_df.drop(['isAnchor','start_time', 'end_time', 'VesselName','nearestPort', 'terminal_encoded', 'start_time_anchor', 'end_time_anchor', 'start_time_anchor_int', 'end_time_anchor_int'], axis = 1)\n",
    "X = X.fillna(-5)\n",
    "y = merged_df[['terminal_encoded', 'isAnchor']]\n",
    "#y = df['nearestPort']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the MultiOutputRegressor with RandomForestRegressor\n",
    "regr_multirf = MultiOutputRegressor(RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "regr_multirf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = regr_multirf.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "# Now, you can use the trained model to make predictions for a new arrived vessel\n",
    "#new_vessel_features = pd.DataFrame({'additional_feature1': [22], 'additional_feature2': [14]})\n",
    "#new_predictions = regr_multirf.predict(new_vessel_features)\n",
    "\n",
    "# Extracting predictions for the new vessel\n",
    "#predicted_terminal, predicted_start_time_anchor, predicted_end_time_anchor = new_predictions[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "883af6ed-6243-461c-ab6d-0ae01e704c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-01 22:14:10.468679: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-01 22:14:10.475490: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-01-01 22:14:10.475515: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost - Forecasting for Length_NY_RedHook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/naristov/.local/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [22:14:13] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost - Forecasting for count_NY_RedHook\n",
      "       Length_NY_RedHook  count_NY_RedHook\n",
      "59093          39.892029          2.866467\n",
      "59094          39.892029          2.866467\n",
      "59095          39.892029          2.866467\n",
      "59096          39.892029          2.866467\n",
      "59097          39.892029          2.866467\n",
      "...                  ...               ...\n",
      "73862           4.278447          2.695386\n",
      "73863           4.278447          2.695386\n",
      "73864           4.278447          2.695386\n",
      "73865           4.278447          2.695386\n",
      "73866           4.278447          2.695386\n",
      "\n",
      "[14774 rows x 2 columns]\n",
      "--------------------------\n",
      "MSE for count is: 3432.4074921908677\n",
      "MAPE for length is: 133.58393965554936\n",
      "MAPE for length is: Length_NY_RedHook    8.382278e+12\n",
      "count_NY_RedHook     2.152388e+12\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pmdarima import auto_arima\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from joblib import dump\n",
    "from tensorflow.keras.models import load_model\n",
    "from joblib import load\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "import NYStatModel as ny1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e41b4cbc-38e3-4622-96f5-a059db1c38a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'NYStatModel' has no attribute 'runXGBoostMerged'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m train[features_columns], train[target_columns]\n\u001b[1;32m     13\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m test[features_columns], test[target_columns]\n\u001b[0;32m---> 14\u001b[0m length_forecast \u001b[38;5;241m=\u001b[39m \u001b[43mny1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunXGBoostMerged\u001b[49m(X_train, X_test, y_train, y_test, terminal)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAPE for length is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_absolute_percentage_error(y_test, length_forecast)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'NYStatModel' has no attribute 'runXGBoostMerged'"
     ]
    }
   ],
   "source": [
    "if (1==1):    \n",
    "    terminal = 'RedHook'\n",
    "    df_merged, features_columns = ny1.dataPreparation()\n",
    "    train_size = int(0.8 * len(df_merged))\n",
    "    train, test = df_merged.iloc[:train_size], df_merged.iloc[train_size:]\n",
    "\n",
    "    length_forecasts = pd.DataFrame(index=test.index)\n",
    "    count_forecasts = pd.DataFrame(index=test.index)\n",
    "    mse_results = {'LSTM':None,'ExponentialSmoothing':None, 'AutoARIMA':None }\n",
    "    target_columns = [f'Length_NY_{terminal}', f'count_NY_{terminal}']\n",
    "\n",
    "    X_train, y_train = train[features_columns], train[target_columns]\n",
    "    X_test, y_test = test[features_columns], test[target_columns]\n",
    "    length_forecast = ny1.runXGBoostMerged(X_train, X_test, y_train, y_test, terminal)\n",
    "\n",
    "    print(f\"MAPE for length is: {mean_absolute_percentage_error(y_test, length_forecast)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86d27aaa-4e43-4f74-aef3-22736b94f01c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"6\" halign=\"left\">Length</th>\n",
       "      <th colspan=\"6\" halign=\"left\">count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nearestPort</th>\n",
       "      <th>NY_APM</th>\n",
       "      <th>NY_LibertyB</th>\n",
       "      <th>NY_LibertyNY</th>\n",
       "      <th>NY_Maher</th>\n",
       "      <th>NY_Newark</th>\n",
       "      <th>NY_RedHook</th>\n",
       "      <th>NY_APM</th>\n",
       "      <th>NY_LibertyB</th>\n",
       "      <th>NY_LibertyNY</th>\n",
       "      <th>NY_Maher</th>\n",
       "      <th>NY_Newark</th>\n",
       "      <th>NY_RedHook</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hour</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-02T00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>294.13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02T01</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>294.13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02T02</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>294.13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02T03</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>294.13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02T04</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>294.13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-30T18</th>\n",
       "      <td>300.0</td>\n",
       "      <td>366.0</td>\n",
       "      <td>207.00</td>\n",
       "      <td>1262.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-30T19</th>\n",
       "      <td>635.0</td>\n",
       "      <td>366.0</td>\n",
       "      <td>207.00</td>\n",
       "      <td>1262.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-30T20</th>\n",
       "      <td>635.0</td>\n",
       "      <td>366.0</td>\n",
       "      <td>207.00</td>\n",
       "      <td>1262.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-30T22</th>\n",
       "      <td>635.0</td>\n",
       "      <td>366.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1262.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-30T23</th>\n",
       "      <td>635.0</td>\n",
       "      <td>366.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1262.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73867 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Length                                                         \\\n",
       "nearestPort   NY_APM NY_LibertyB NY_LibertyNY NY_Maher NY_Newark NY_RedHook   \n",
       "Hour                                                                          \n",
       "2015-01-02T00    0.0         0.0       294.13      0.0       0.0        0.0   \n",
       "2015-01-02T01    0.0         0.0       294.13      0.0       0.0        0.0   \n",
       "2015-01-02T02    0.0         0.0       294.13      0.0       0.0        0.0   \n",
       "2015-01-02T03    0.0         0.0       294.13      0.0       0.0        0.0   \n",
       "2015-01-02T04    0.0         0.0       294.13      0.0       0.0        0.0   \n",
       "...              ...         ...          ...      ...       ...        ...   \n",
       "2023-09-30T18  300.0       366.0       207.00   1262.0     332.0        0.0   \n",
       "2023-09-30T19  635.0       366.0       207.00   1262.0     332.0        0.0   \n",
       "2023-09-30T20  635.0       366.0       207.00   1262.0     332.0        0.0   \n",
       "2023-09-30T22  635.0       366.0         0.00   1262.0     332.0        0.0   \n",
       "2023-09-30T23  635.0       366.0         0.00   1262.0     332.0        0.0   \n",
       "\n",
       "               count                                                         \n",
       "nearestPort   NY_APM NY_LibertyB NY_LibertyNY NY_Maher NY_Newark NY_RedHook  \n",
       "Hour                                                                         \n",
       "2015-01-02T00      0           0            1        0         0          0  \n",
       "2015-01-02T01      0           0            1        0         0          0  \n",
       "2015-01-02T02      0           0            1        0         0          0  \n",
       "2015-01-02T03      0           0            1        0         0          0  \n",
       "2015-01-02T04      0           0            1        0         0          0  \n",
       "...              ...         ...          ...      ...       ...        ...  \n",
       "2023-09-30T18      1           1            1        4         1          0  \n",
       "2023-09-30T19      2           1            1        4         1          0  \n",
       "2023-09-30T20      2           1            1        4         1          0  \n",
       "2023-09-30T22      2           1            0        4         1          0  \n",
       "2023-09-30T23      2           1            0        4         1          0  \n",
       "\n",
       "[73867 rows x 12 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kk = ny1.berthStatsNY.copy()\n",
    "kk[kk['nearestPort']=='NY_RedHook']\n",
    "\n",
    "df_pivoted = kk.pivot_table(index='Hour', columns='nearestPort', values=['count', 'Length'], aggfunc='sum', fill_value=0)\n",
    "df_pivoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b067ed64-b863-4ac9-bceb-6a6d6299fb37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>index</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Hour</th>\n",
       "      <th>nearestPort</th>\n",
       "      <th>count</th>\n",
       "      <th>Length</th>\n",
       "      <th>Width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>520</td>\n",
       "      <td>275931</td>\n",
       "      <td>3737</td>\n",
       "      <td>2015-01-12T17</td>\n",
       "      <td>NY_RedHook</td>\n",
       "      <td>1</td>\n",
       "      <td>210.07</td>\n",
       "      <td>30.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>523</td>\n",
       "      <td>275948</td>\n",
       "      <td>3754</td>\n",
       "      <td>2015-01-12T18</td>\n",
       "      <td>NY_RedHook</td>\n",
       "      <td>1</td>\n",
       "      <td>210.07</td>\n",
       "      <td>30.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>526</td>\n",
       "      <td>275965</td>\n",
       "      <td>3771</td>\n",
       "      <td>2015-01-12T19</td>\n",
       "      <td>NY_RedHook</td>\n",
       "      <td>1</td>\n",
       "      <td>210.07</td>\n",
       "      <td>30.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>529</td>\n",
       "      <td>275982</td>\n",
       "      <td>3788</td>\n",
       "      <td>2015-01-12T20</td>\n",
       "      <td>NY_RedHook</td>\n",
       "      <td>1</td>\n",
       "      <td>210.07</td>\n",
       "      <td>30.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>532</td>\n",
       "      <td>275998</td>\n",
       "      <td>3804</td>\n",
       "      <td>2015-01-12T21</td>\n",
       "      <td>NY_RedHook</td>\n",
       "      <td>1</td>\n",
       "      <td>210.07</td>\n",
       "      <td>30.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251679</th>\n",
       "      <td>251679</td>\n",
       "      <td>1062374</td>\n",
       "      <td>14978</td>\n",
       "      <td>2023-09-30T13</td>\n",
       "      <td>NY_RedHook</td>\n",
       "      <td>1</td>\n",
       "      <td>166.00</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251684</th>\n",
       "      <td>251684</td>\n",
       "      <td>1062395</td>\n",
       "      <td>14999</td>\n",
       "      <td>2023-09-30T14</td>\n",
       "      <td>NY_RedHook</td>\n",
       "      <td>1</td>\n",
       "      <td>166.00</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251692</th>\n",
       "      <td>251692</td>\n",
       "      <td>1062416</td>\n",
       "      <td>15020</td>\n",
       "      <td>2023-09-30T15</td>\n",
       "      <td>NY_RedHook</td>\n",
       "      <td>1</td>\n",
       "      <td>166.00</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251696</th>\n",
       "      <td>251696</td>\n",
       "      <td>1062437</td>\n",
       "      <td>15041</td>\n",
       "      <td>2023-09-30T16</td>\n",
       "      <td>NY_RedHook</td>\n",
       "      <td>1</td>\n",
       "      <td>166.00</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251702</th>\n",
       "      <td>251702</td>\n",
       "      <td>1062458</td>\n",
       "      <td>15062</td>\n",
       "      <td>2023-09-30T17</td>\n",
       "      <td>NY_RedHook</td>\n",
       "      <td>1</td>\n",
       "      <td>166.00</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7742 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0.1    index  Unnamed: 0           Hour nearestPort  count  \\\n",
       "520              520   275931        3737  2015-01-12T17  NY_RedHook      1   \n",
       "523              523   275948        3754  2015-01-12T18  NY_RedHook      1   \n",
       "526              526   275965        3771  2015-01-12T19  NY_RedHook      1   \n",
       "529              529   275982        3788  2015-01-12T20  NY_RedHook      1   \n",
       "532              532   275998        3804  2015-01-12T21  NY_RedHook      1   \n",
       "...              ...      ...         ...            ...         ...    ...   \n",
       "251679        251679  1062374       14978  2023-09-30T13  NY_RedHook      1   \n",
       "251684        251684  1062395       14999  2023-09-30T14  NY_RedHook      1   \n",
       "251692        251692  1062416       15020  2023-09-30T15  NY_RedHook      1   \n",
       "251696        251696  1062437       15041  2023-09-30T16  NY_RedHook      1   \n",
       "251702        251702  1062458       15062  2023-09-30T17  NY_RedHook      1   \n",
       "\n",
       "        Length  Width  \n",
       "520     210.07   30.2  \n",
       "523     210.07   30.2  \n",
       "526     210.07   30.2  \n",
       "529     210.07   30.2  \n",
       "532     210.07   30.2  \n",
       "...        ...    ...  \n",
       "251679  166.00   25.0  \n",
       "251684  166.00   25.0  \n",
       "251692  166.00   25.0  \n",
       "251696  166.00   25.0  \n",
       "251702  166.00   25.0  \n",
       "\n",
       "[7742 rows x 8 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kk[kk['nearestPort']=='NY_RedHook']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab8ec41d-9f3d-41df-a2d1-53a7c3e26c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9124 entries, 6346 to 7270\n",
      "Data columns (total 105 columns):\n",
      " #    Column                                   Non-Null Count  Dtype  \n",
      "---   ------                                   --------------  -----  \n",
      " 0    MMSI                                     9124 non-null   float64\n",
      " 1    Length_x                                 9124 non-null   float64\n",
      " 2    Width_x                                  9124 non-null   float64\n",
      " 3    TimeSpent                                9124 non-null   float64\n",
      " 4    MMSI_anchor                              9124 non-null   float64\n",
      " 5    Length_anchor                            9124 non-null   float64\n",
      " 6    Width_anchor                             9124 non-null   float64\n",
      " 7    TimeSpent_anchor                         9124 non-null   float64\n",
      " 8    Date_difference                          9124 non-null   float64\n",
      " 9    Length_y                                 9124 non-null   float64\n",
      " 10   Width_y                                  9124 non-null   float64\n",
      " 11   operator_APL LLC                         9124 non-null   uint8  \n",
      " 12   operator_Anglo-Eastern Ship Management   9124 non-null   uint8  \n",
      " 13   operator_Artran Ltd                      9124 non-null   uint8  \n",
      " 14   operator_CMA CGM SA The French Line      9124 non-null   uint8  \n",
      " 15   operator_COSCO Shipping Lines Co Ltd     9124 non-null   uint8  \n",
      " 16   operator_Compagnie Maritime Marfret SA   9124 non-null   uint8  \n",
      " 17   operator_Conbulk Shipping SA             9124 non-null   uint8  \n",
      " 18   operator_Cosmoship Management SA         9124 non-null   uint8  \n",
      " 19   operator_Costamare Shipping Co SA        9124 non-null   uint8  \n",
      " 20   operator_Crowley Caribbean Services LLC  9124 non-null   uint8  \n",
      " 21   operator_Crowley Liner Services Inc      9124 non-null   uint8  \n",
      " 22   operator_Crowley Maritime Corp           9124 non-null   uint8  \n",
      " 23   operator_Dole Ocean Cargo Express Inc    9124 non-null   uint8  \n",
      " 24   operator_Eimskip Island ehf              9124 non-null   uint8  \n",
      " 25   operator_Ellerman City Liners Ltd        9124 non-null   uint8  \n",
      " 26   operator_Emirates Shipping Line DMCEST   9124 non-null   uint8  \n",
      " 27   operator_Evergreen Marine Asia Pte Ltd   9124 non-null   uint8  \n",
      " 28   operator_Evergreen Marine Corp           9124 non-null   uint8  \n",
      " 29   operator_FESCO                           9124 non-null   uint8  \n",
      " 30   operator_Global Feeder Shipping Pte Ltd  9124 non-null   uint8  \n",
      " 31   operator_Gold Star Line Ltd              9124 non-null   uint8  \n",
      " 32   operator_Great White Fleet Ltd           9124 non-null   uint8  \n",
      " 33   operator_HMM Co Ltd                      9124 non-null   uint8  \n",
      " 34   operator_Hamburg Sudamerikanische        9124 non-null   uint8  \n",
      " 35   operator_Hapag-Lloyd AG                  9124 non-null   uint8  \n",
      " 36   operator_Ignazio Messina & C SpA         9124 non-null   uint8  \n",
      " 37   operator_Independent Container Line Ltd  9124 non-null   uint8  \n",
      " 38   operator_Italia Marittima SpA            9124 non-null   uint8  \n",
      " 39   operator_Jupiter 18 Ltd                  9124 non-null   uint8  \n",
      " 40   operator_King Ocean Services Ltd         9124 non-null   uint8  \n",
      " 41   operator_MSC Mediterranean Shipping Co   9124 non-null   uint8  \n",
      " 42   operator_MSC Shipmanagement Ltd          9124 non-null   uint8  \n",
      " 43   operator_Maersk A/S                      9124 non-null   uint8  \n",
      " 44   operator_Maersk Line Ltd-USA             9124 non-null   uint8  \n",
      " 45   operator_Maersk Sealand Europe & Med     9124 non-null   uint8  \n",
      " 46   operator_Matson Navigation Co Inc        9124 non-null   uint8  \n",
      " 47   operator_Mediterranean Shipping Co Srl   9124 non-null   uint8  \n",
      " 48   operator_Nile Dutch Africa Line BV       9124 non-null   uint8  \n",
      " 49   operator_Nordic Hamburg Shipmgmt GmbH    9124 non-null   uint8  \n",
      " 50   operator_Number 9 Shipping Ltd           9124 non-null   uint8  \n",
      " 51   operator_Ocean Network Express Pte Ltd   9124 non-null   uint8  \n",
      " 52   operator_Oceonix Services Ltd            9124 non-null   uint8  \n",
      " 53   operator_Oman Shipping Co SAOC           9124 non-null   uint8  \n",
      " 54   operator_Orient Express Lines Inc        9124 non-null   uint8  \n",
      " 55   operator_Orient Overseas Container Line  9124 non-null   uint8  \n",
      " 56   operator_PanAsia Shipping Co Ltd         9124 non-null   uint8  \n",
      " 57   operator_RHL Reederei Hamburger Lloyd    9124 non-null   uint8  \n",
      " 58   operator_Reel Shipping LLC               9124 non-null   uint8  \n",
      " 59   operator_RifLine Worldwide Logistics     9124 non-null   uint8  \n",
      " 60   operator_SFL Management Singapore        9124 non-null   uint8  \n",
      " 61   operator_SITC Container Lines Co Ltd     9124 non-null   uint8  \n",
      " 62   operator_Safeen Feeders 2 Ltd            9124 non-null   uint8  \n",
      " 63   operator_Safmarine Container Lines       9124 non-null   uint8  \n",
      " 64   operator_Samudera Indonesia Tbk PT       9124 non-null   uint8  \n",
      " 65   operator_Sea Lead Shipping Pte Ltd       9124 non-null   uint8  \n",
      " 66   operator_Seaboard Marine Ltd Inc         9124 non-null   uint8  \n",
      " 67   operator_Sealand Europe A/S              9124 non-null   uint8  \n",
      " 68   operator_Sealand Maersk Asia Pte Ltd     9124 non-null   uint8  \n",
      " 69   operator_Shanghai Ocean Shipping Co Ltd  9124 non-null   uint8  \n",
      " 70   operator_Shoei Kisen KK                  9124 non-null   uint8  \n",
      " 71   operator_Sinokor Merchant Marine Co Ltd  9124 non-null   uint8  \n",
      " 72   operator_Tailwind Shipping Lines GmbH    9124 non-null   uint8  \n",
      " 73   operator_Tom Worden GmbH & Co KG         9124 non-null   uint8  \n",
      " 74   operator_Transfar Shipping Pte Ltd       9124 non-null   uint8  \n",
      " 75   operator_Turkon Konteyner Tasimacilik    9124 non-null   uint8  \n",
      " 76   operator_Unifeeder A/S                   9124 non-null   uint8  \n",
      " 77   operator_Universal Shipping Alliance     9124 non-null   uint8  \n",
      " 78   operator_Wan Hai Lines Ltd               9124 non-null   uint8  \n",
      " 79   operator_Westwood Shipping Lines Inc     9124 non-null   uint8  \n",
      " 80   operator_X-Press Feeders                 9124 non-null   uint8  \n",
      " 81   operator_Yang Ming Marine Transport      9124 non-null   uint8  \n",
      " 82   operator_Zim Integrated Shipping Servs   9124 non-null   uint8  \n",
      " 83   operator_Zodiac Maritime Ltd             9124 non-null   uint8  \n",
      " 84   Hour                                     9124 non-null   int64  \n",
      " 85   Length_NY_APM                            9124 non-null   float64\n",
      " 86   Length_NY_LibertyB                       9124 non-null   float64\n",
      " 87   Length_NY_LibertyNY                      9124 non-null   float64\n",
      " 88   Length_NY_Maher                          9124 non-null   float64\n",
      " 89   Length_NY_Newark                         9124 non-null   float64\n",
      " 90   Length_NY_RedHook                        9124 non-null   float64\n",
      " 91   count_NY_APM                             9124 non-null   float64\n",
      " 92   count_NY_LibertyB                        9124 non-null   float64\n",
      " 93   count_NY_LibertyNY                       9124 non-null   float64\n",
      " 94   count_NY_Maher                           9124 non-null   float64\n",
      " 95   count_NY_Newark                          9124 non-null   float64\n",
      " 96   count_NY_RedHook                         9124 non-null   float64\n",
      " 97   count                                    9124 non-null   float64\n",
      " 98   count_harbor                             9124 non-null   float64\n",
      " 99   Length                                   9124 non-null   float64\n",
      " 100  Width                                    9124 non-null   float64\n",
      " 101  day_of_week                              9124 non-null   float64\n",
      " 102  month                                    9124 non-null   float64\n",
      " 103  start_time_anchor_int                    9124 non-null   int64  \n",
      " 104  end_time_anchor_int                      9124 non-null   int64  \n",
      "dtypes: float64(29), int64(3), uint8(73)\n",
      "memory usage: 2.9 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/slurm_tmp/24594166.0.0/ipykernel_2275255/591082021.py:1: FutureWarning: null_counts is deprecated. Use show_counts instead\n",
      "  X_train.info(verbose=True, null_counts=True)\n"
     ]
    }
   ],
   "source": [
    "X_train.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a643176a-95bd-4ee7-b171-6ebe41683845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data with exogenous features\n",
    "df_exog = pd.DataFrame({\n",
    "    'date': pd.date_range(start='2022-01-01', end='2022-01-31', freq='D'),\n",
    "    'exog_var1': [10, 12, 15, 18, 20, 22, 25, 28, 30, 32, 35, 38, 40, 42, 45, 48, 50, 52, 55, 58, 60, 62, 65, 68, 70, 72, 75, 78, 80, 82],\n",
    "    'exog_var2': [50, 48, 45, 42, 40, 38, 35, 32, 30, 28, 25, 22, 20, 18, 15, 12, 10, 8, 5, 2, 0, -2, -5, -8, -10, -12, -15, -18, -20, -22]\n",
    "})\n",
    "\n",
    "# Combine endogenous and exogenous variables\n",
    "df_combined = pd.merge(df_var, df_exog, on='date')\n",
    "\n",
    "# Split the data into training and testing sets (80/20)\n",
    "train_size = int(len(df_combined) * 0.8)\n",
    "train, test = df_combined[:train_size], df_combined[train_size:]\n",
    "\n",
    "# Create and fit a VAR model with exogenous variables\n",
    "model = VAR(train[['var1', 'var2']])\n",
    "results = model.fit(deterministic_type='const', exog=train[['exog_var1', 'exog_var2']])\n",
    "\n",
    "# Forecast for the test set\n",
    "forecast = results.forecast(train[['var1', 'var2']].values, steps=len(test), exog_future=test[['exog_var1', 'exog_var2']])\n",
    "\n",
    "# Convert the forecast array to a DataFrame with appropriate column names\n",
    "forecast_df = pd.DataFrame(forecast, columns=['var1', 'var2'])\n",
    "\n",
    "# Print or analyze the forecasted values\n",
    "print(forecast_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35583e8a-17c9-4c2b-869a-3fce607c3f51",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "multiclass-multioutput format is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m predicted \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpathOutput\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/count_forecasts_LibertyB_arima.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Binary Classification Metrics\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m precision, recall, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_curve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m fpr, tpr, _ \u001b[38;5;241m=\u001b[39m roc_curve(pd\u001b[38;5;241m.\u001b[39mget_dummies(actual)\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], pd\u001b[38;5;241m.\u001b[39mget_dummies(predicted)\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# AUC-ROC and AUC-PR\u001b[39;00m\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2022b/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:878\u001b[0m, in \u001b[0;36mprecision_recall_curve\u001b[0;34m(y_true, probas_pred, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprecision_recall_curve\u001b[39m(y_true, probas_pred, \u001b[38;5;241m*\u001b[39m, pos_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    798\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute precision-recall pairs for different probability thresholds.\u001b[39;00m\n\u001b[1;32m    799\u001b[0m \n\u001b[1;32m    800\u001b[0m \u001b[38;5;124;03m    Note: this implementation is restricted to the binary classification task.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;124;03m    array([0.1 , 0.35, 0.4 , 0.8 ])\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m     fps, tps, thresholds \u001b[38;5;241m=\u001b[39m \u001b[43m_binary_clf_curve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprobas_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    882\u001b[0m     ps \u001b[38;5;241m=\u001b[39m tps \u001b[38;5;241m+\u001b[39m fps\n\u001b[1;32m    883\u001b[0m     \u001b[38;5;66;03m# Initialize the result array with zeros to make sure that precision[ps == 0]\u001b[39;00m\n\u001b[1;32m    884\u001b[0m     \u001b[38;5;66;03m# does not contain uninitialized values.\u001b[39;00m\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2022b/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:749\u001b[0m, in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    747\u001b[0m y_type \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m pos_label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[0;32m--> 749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m format is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[1;32m    751\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[1;32m    752\u001b[0m y_true \u001b[38;5;241m=\u001b[39m column_or_1d(y_true)\n",
      "\u001b[0;31mValueError\u001b[0m: multiclass-multioutput format is not supported"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier  # Replace with your model\n",
    "\n",
    "userName = 'naristov'\n",
    "pathOutput = f\"/home/gridsan/{userName}/Modelling/Output\"\n",
    "\n",
    "actual = pd.read_csv(f\"{pathOutput}/count_forecasts_LibertyB_y.csv\")\n",
    "predicted = pd.read_csv(f\"{pathOutput}/count_forecasts_LibertyB_arima.csv\")\n",
    "# Binary Classification Metrics\n",
    "precision, recall, _ = precision_recall_curve(actual, predicted, pos_label=2)\n",
    "fpr, tpr, _ = roc_curve(pd.get_dummies(actual).iloc[:, -1], pd.get_dummies(predicted).iloc[:, -1])\n",
    "\n",
    "# AUC-ROC and AUC-PR\n",
    "roc_auc = auc(fpr, tpr)\n",
    "pr_auc = average_precision_score(pd.get_dummies(actual).iloc[:, -1], pd.get_dummies(predicted).iloc[:, -1])\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'AUC = {roc_auc:.2f}')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Plot Precision-Recall Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, color='darkorange', lw=2, label=f'AUC = {pr_auc:.2f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()\n",
    "\n",
    "# Cross-Validation\n",
    "#model = RandomForestClassifier()  # Replace with your model\n",
    "#cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "#predicted_cv = cross_val_predict(model, pd.DataFrame(np.random.rand(100, 10)), actual, cv=cv)\n",
    "\n",
    "# Additional Analysis with Predictions from Cross-Validation\n",
    "# (replace with your own analysis or evaluation metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0930f605-3a94-46ca-9329-71e726f92deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-02 22:41:43.062785: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-02 22:41:43.074525: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-01-02 22:41:43.074599: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import NYStatModel as ny\n",
    "df_merged, features_columns = ny.dataPreparation(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3f44f10-3f95-4cc4-b852-8642ac9e33df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 74257 entries, 0 to 74256\n",
      "Data columns (total 45 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   Hour                        74257 non-null  int64  \n",
      " 1   Length_Boston               74257 non-null  float64\n",
      " 2   Length_NY_APM               74257 non-null  float64\n",
      " 3   Length_NY_LibertyB          74257 non-null  float64\n",
      " 4   Length_NY_LibertyNY         74257 non-null  float64\n",
      " 5   Length_NY_Maher             74257 non-null  float64\n",
      " 6   Length_NY_Newark            74257 non-null  float64\n",
      " 7   Length_NY_RedHook           74257 non-null  float64\n",
      " 8   count_Boston                74257 non-null  int64  \n",
      " 9   count_NY_APM                74257 non-null  int64  \n",
      " 10  count_NY_LibertyB           74257 non-null  int64  \n",
      " 11  count_NY_LibertyNY          74257 non-null  int64  \n",
      " 12  count_NY_Maher              74257 non-null  int64  \n",
      " 13  count_NY_Newark             74257 non-null  int64  \n",
      " 14  count_NY_RedHook            74257 non-null  int64  \n",
      " 15  Unnamed: 0.1                74257 non-null  float64\n",
      " 16  index                       74257 non-null  float64\n",
      " 17  Unnamed: 0                  74257 non-null  float64\n",
      " 18  count                       74257 non-null  float64\n",
      " 19  Unnamed: 0.1_harbor_NY      74257 non-null  float64\n",
      " 20  index_harbor_NY             74257 non-null  float64\n",
      " 21  Unnamed: 0_harbor_NY        74257 non-null  float64\n",
      " 22  count_harbor_NY             74257 non-null  float64\n",
      " 23  Length                      74257 non-null  float64\n",
      " 24  Width                       74257 non-null  float64\n",
      " 25  Unnamed: 0.1_anchor_Boston  74257 non-null  float64\n",
      " 26  index_anchor_Boston         74257 non-null  float64\n",
      " 27  Unnamed: 0_anchor_Boston    74257 non-null  float64\n",
      " 28  count_anchor_Boston         74257 non-null  float64\n",
      " 29  Unnamed: 0.1_harbor_Boston  74257 non-null  float64\n",
      " 30  index_harbor_Boston         74257 non-null  float64\n",
      " 31  Unnamed: 0_harbor_Boston    74257 non-null  float64\n",
      " 32  count_harbor_Boston         74257 non-null  float64\n",
      " 33  Length_harbor_Boston        74257 non-null  float64\n",
      " 34  Width_harbor_Boston         74257 non-null  float64\n",
      " 35  hour_of_day                 74257 non-null  int64  \n",
      " 36  day_of_week                 74257 non-null  int64  \n",
      " 37  month                       74257 non-null  int64  \n",
      " 38  diff_to_prev_NY_APM         74257 non-null  float64\n",
      " 39  diff_to_prev_NY_LibertyB    74257 non-null  float64\n",
      " 40  diff_to_prev_NY_LibertyNY   74257 non-null  float64\n",
      " 41  diff_to_prev_NY_Maher       74257 non-null  float64\n",
      " 42  diff_to_prev_NY_Newark      74257 non-null  float64\n",
      " 43  diff_to_prev_NY_RedHook     74257 non-null  float64\n",
      " 44  diff_to_prev_Boston         74257 non-null  float64\n",
      "dtypes: float64(34), int64(11)\n",
      "memory usage: 26.1 MB\n"
     ]
    }
   ],
   "source": [
    "df_merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d33725f0-edab-417c-a9ad-39267cec3b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: count, NY_APM, arima \n",
      "1: count, NY_APM, expSmooth \n",
      "2: count, NY_APM, LSTM \n",
      "3: count, NY_APM, XGBoost \n",
      "4: count, NY_APM, VAR \n",
      "5: count, NY_LibertyB, arima \n",
      "6: count, NY_LibertyB, expSmooth \n",
      "7: count, NY_LibertyB, LSTM \n",
      "8: count, NY_LibertyB, XGBoost \n",
      "9: count, NY_LibertyB, VAR \n",
      "10: count, NY_LibertyNY, arima \n",
      "11: count, NY_LibertyNY, expSmooth \n",
      "12: count, NY_LibertyNY, LSTM \n",
      "13: count, NY_LibertyNY, XGBoost \n",
      "14: count, NY_LibertyNY, VAR \n",
      "15: count, NY_Maher, arima \n",
      "16: count, NY_Maher, expSmooth \n",
      "17: count, NY_Maher, LSTM \n",
      "18: count, NY_Maher, XGBoost \n",
      "19: count, NY_Maher, VAR \n",
      "20: count, NY_Newark, arima \n",
      "21: count, NY_Newark, expSmooth \n",
      "22: count, NY_Newark, LSTM \n",
      "23: count, NY_Newark, XGBoost \n",
      "24: count, NY_Newark, VAR \n",
      "25: count, NY_RedHook, arima \n",
      "26: count, NY_RedHook, expSmooth \n",
      "27: count, NY_RedHook, LSTM \n",
      "28: count, NY_RedHook, XGBoost \n",
      "29: count, NY_RedHook, VAR \n",
      "30: count, Boston, arima \n",
      "31: count, Boston, expSmooth \n",
      "32: count, Boston, LSTM \n",
      "33: count, Boston, XGBoost \n",
      "34: count, Boston, VAR \n",
      "35: diff, NY_APM, arima \n",
      "36: diff, NY_APM, expSmooth \n",
      "37: diff, NY_APM, LSTM \n",
      "38: diff, NY_APM, XGBoost \n",
      "39: diff, NY_APM, VAR \n",
      "40: diff, NY_LibertyB, arima \n",
      "41: diff, NY_LibertyB, expSmooth \n",
      "42: diff, NY_LibertyB, LSTM \n",
      "43: diff, NY_LibertyB, XGBoost \n",
      "44: diff, NY_LibertyB, VAR \n",
      "45: diff, NY_LibertyNY, arima \n",
      "46: diff, NY_LibertyNY, expSmooth \n",
      "47: diff, NY_LibertyNY, LSTM \n",
      "48: diff, NY_LibertyNY, XGBoost \n",
      "49: diff, NY_LibertyNY, VAR \n",
      "50: diff, NY_Maher, arima \n",
      "51: diff, NY_Maher, expSmooth \n",
      "52: diff, NY_Maher, LSTM \n",
      "53: diff, NY_Maher, XGBoost \n",
      "54: diff, NY_Maher, VAR \n",
      "55: diff, NY_Newark, arima \n",
      "56: diff, NY_Newark, expSmooth \n",
      "57: diff, NY_Newark, LSTM \n",
      "58: diff, NY_Newark, XGBoost \n",
      "59: diff, NY_Newark, VAR \n",
      "60: diff, NY_RedHook, arima \n",
      "61: diff, NY_RedHook, expSmooth \n",
      "62: diff, NY_RedHook, LSTM \n",
      "63: diff, NY_RedHook, XGBoost \n",
      "64: diff, NY_RedHook, VAR \n",
      "65: diff, Boston, arima \n",
      "66: diff, Boston, expSmooth \n",
      "67: diff, Boston, LSTM \n"
     ]
    }
   ],
   "source": [
    "mods = ['count', 'diff']\n",
    "terminals = ['NY_APM', 'NY_LibertyB', 'NY_LibertyNY', 'NY_Maher', 'NY_Newark', 'NY_RedHook', 'Boston']\n",
    "Models_terminal = ['arima', 'expSmooth', 'LSTM', 'XGBoost'] \n",
    "\n",
    "Models_grouped = ['LSTM', 'XGBoost', 'VAR'] \n",
    "\n",
    "NYterminals = ['NY_APM', 'NY_LibertyB', 'NY_LibertyNY', 'NY_Maher', 'NY_Newark', 'NY_RedHook']\n",
    "EastCostTerminals = ['NY_APM', 'NY_LibertyB', 'NY_LibertyNY', 'NY_Maher', 'NY_Newark', 'NY_RedHook', 'Boston']\n",
    "\n",
    "GroupedTerminals = [NYterminals, EastCostTerminals]\n",
    "\n",
    "models_per_terminal = len(Models_terminal)\n",
    "variables_count = len(mods)\n",
    "single_terminal_models = models_per_terminal * len(terminals)\n",
    "grouped_terminal_models = len(Models_grouped) * len (GroupedTerminals)\n",
    "\n",
    "for i in range(68):\n",
    "    if i < single_terminal_models * variables_count:\n",
    "        mod = mods [i // single_terminal_models]\n",
    "        k = i % single_terminal_models\n",
    "        terminal = terminals[k // models_per_terminal]\n",
    "        model = Models_terminal[k % models_per_terminal]\n",
    "        print (f\"{i}: {mod}, {terminal}, {model} \")\n",
    "    else:\n",
    "        k = i - single_terminal_models * variables_count\n",
    "        mod = mods [k // grouped_terminal_models]\n",
    "        l = k % grouped_terminal_models\n",
    "        groupedTerminal = GroupedTerminals[l // len(Models_grouped)]\n",
    "        model = Models_grouped[l % len(Models_grouped)]\n",
    "        print (f\"{i}: {mod}, {groupedTerminal}, {model} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66904919-6b76-4df1-b26c-091dd4955ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
